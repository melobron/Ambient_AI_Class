{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-38Nv5MBNYLI"
   },
   "source": [
    "# TensorFlow Lite model optimization\n",
    "\n",
    "TensorFlow Lite에서는 edge device에서 모델의 inference를 효율적으로 하기 위한 방법인 quantization과 pruning을 이용할 수 있는 toolkit을 제공하고 있습니다. 이번 실습에서는 해당 toolkit을 이용해 볼 것입니다.\n",
    "\n",
    "더 자세한 내용은 document를 참고 해주세요.\n",
    "- https://www.tensorflow.org/model_optimization/guide\n",
    "- https://www.tensorflow.org/lite/performance/model_optimization\n",
    "- TFLite https://www.tensorflow.org/lite/guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmO6ifCcNYLK"
   },
   "source": [
    "## 0. What is TFLite?\n",
    "\n",
    "TensorFlow Lite는 TensorFlow로 학습된 모델을 tflite이라는 포맷으로 바꿔 줌으로써 모바일 장치, 임베디드 장치 등의 가벼운 장치에서 모델을 inference 할 수 있게 해주는 플랫폼 이다.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/37704174/115159215-fccd0800-a0cc-11eb-817b-645f96c76966.png\" width=\"400\" height=\"400\"/>  \n",
    "\n",
    "\n",
    "TensorFlow는 모델을 파일로 저장할 때는 [Protocol Buffer](https://developers.google.com/protocol-buffers)라는 포맷으로 저장을 하고, TensorFlow Lite의 모델은 [FlatBuffer](https://google.github.io/flatbuffers/)라는 포맷으로 저장을 합니다. 두 포맷의 가장 큰 차이점은 FlatBuffer는 serialized 파일을 추가적인 메모리 할당 없이 deserialize를 할 수 있기 때문에 빠르게 파일에 있는 데이터에 접근할 수 있다.\n",
    "\n",
    "\n",
    "TFLite 모델로 변환을 할 때, quantization과 pruning 등의 optimization을 할 수 있는데, 이 때의 장점은 다음과 같다.\n",
    "\n",
    "- Size reduction\n",
    "  - smaller storage size, smaler download size, less memory usage\n",
    "- Latency reduction\n",
    "  - computation을 줄여 줌으로써 inference에 걸리는 시간 감소\n",
    "- Accelerator compatibility\n",
    "  - Edge TPU와 같이 특정 hardware는 각각의 quantization 요구 사항을 만족해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oc3kI_bXNYLL"
   },
   "source": [
    "tensorflow 최신 버전 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CHbQNyjRNYLL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Er5ClTbENYLM",
    "outputId": "cb7913ff-6547-4dab-bf06-d0832625f294"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "assert float(tf.__version__[:3]) >= 2.6\n",
    "\n",
    "tf.random.set_seed(1111)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MOi-AOHCNYLM",
    "outputId": "7238f4f6-4e09-4c0c-cc32-a01123a46f64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyFfTRHWNYLN"
   },
   "source": [
    "## 1. Quantization\n",
    "### 1-1. [Post-training quantization](https://blog.tensorflow.org/2019/06/tensorflow-integer-quantization.html)\n",
    "\n",
    "post-training quantization은 학습을 한 다음에 quantization을 하는 방법이다. quantization aware training보다는 accuracy가 좀 떨어질 수 있지만, 학습된 모델을 가져와서 쉽게 쓸 수 있다. tensorflow에서 다음과 같은 방법들을 제공하고 있다.\n",
    "\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/37704174/114367164-2232b100-9bb7-11eb-81e7-b9b8dec09de5.png\" width=\"700\" height=\"700\"/>  \n",
    "- dynamic range quantization: weight은 8 bit integer로 저장. inference 시에는 8bit float로 바뀜. activation은 range에 따라서(dynamic) 8 bit float으로 저장\n",
    "\n",
    "\n",
    "나중에 Edge TPU가 있는 Coral board를 사용할 것이기 때문에, post-training integer quantization을 다뤄 볼 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxO32_RZNYLN"
   },
   "source": [
    "먼저 quantizaion을 할 간단한 모델을 학습한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AyuU6UCINYLP",
    "outputId": "d1e52bc0-65c3-4cd7-85e0-ed1e9cf8b10e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 21s 4ms/step - loss: 0.4777 - accuracy: 0.8306 - val_loss: 0.3759 - val_accuracy: 0.8670\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.3323 - accuracy: 0.8826 - val_loss: 0.3356 - val_accuracy: 0.8808\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2934 - accuracy: 0.8958 - val_loss: 0.2888 - val_accuracy: 0.8948\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2668 - accuracy: 0.9050 - val_loss: 0.2773 - val_accuracy: 0.9003\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2471 - accuracy: 0.9106 - val_loss: 0.2603 - val_accuracy: 0.9047\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2308 - accuracy: 0.9161 - val_loss: 0.2913 - val_accuracy: 0.8927\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2186 - accuracy: 0.9216 - val_loss: 0.2598 - val_accuracy: 0.9070\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 8s 4ms/step - loss: 0.2062 - accuracy: 0.9252 - val_loss: 0.2476 - val_accuracy: 0.9107\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.1957 - accuracy: 0.9288 - val_loss: 0.2393 - val_accuracy: 0.9122\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 8s 4ms/step - loss: 0.1880 - accuracy: 0.9317 - val_loss: 0.2520 - val_accuracy: 0.9085\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2729 - accuracy: 0.9067\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the input image\n",
    "train_images = train_images.astype(np.float32) / 255.0\n",
    "test_images = test_images.astype(np.float32) / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and Train\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,  \n",
    "  epochs=10,\n",
    "  validation_split=0.1\n",
    ")\n",
    "\n",
    "metrics = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test loss: 0.273 | test accuracy: 90.67%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model test loss: {metrics[0]:.03f} | test accuracy: {metrics[1]*100:.02f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AudCskQwNYLQ"
   },
   "source": [
    "학습된 모델을 변환하기 위해서는 [TensorFlow Lite converter](https://www.tensorflow.org/lite/convert)라는 것을 이용해야 합니다. 이것은 TensorFlow 모델을 TensorFlow Lite 모델로 바꿔 줍니다.\n",
    "\n",
    "다음과 같이 tflite 모델로 변환할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gb41bItUNYLQ",
    "outputId": "f708ad1e-1fde-46e9-bddb-df54a9650887"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmp6ob4_9iy\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmp6ob4_9iy\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWv-DJiWNYLQ"
   },
   "source": [
    "변환할 때 아무런 옵션을 주지 않았으므로 이 모델은 단순히 32-bit float tflite 모델입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzTkxHFBNYLQ"
   },
   "source": [
    "### Post-training integer quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Miv-lI8NYLR"
   },
   "source": [
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/37704174/114906428-f9c7e280-9e54-11eb-9038-9854d5c8d741.png\" width=\"700\" height=\"700\"/>  \n",
    "<img src=\"https://user-images.githubusercontent.com/37704174/114907026-9e4a2480-9e55-11eb-980f-55999688c260.png\" width=\"500\" height=\"500\"/>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDfn7MVMNYLR"
   },
   "source": [
    "다음 셀을 실행하면 quantization이 완료 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NJLtl1woNYLR",
    "outputId": "e04550b5-0c99-4282-cad8-1d49edb8d9e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmpbul3oa68\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmpbul3oa68\\assets\n",
      "C:\\Users\\steve\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    }
   ],
   "source": [
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8 # or tf.int8\n",
    "converter.inference_output_type = tf.uint8 # or tf.int8\n",
    "\n",
    "post_quant_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LQXLHXbNYLR"
   },
   "source": [
    "evaluate을 위한 helper function이다. input type을 integer로 했으므로, quantize를 한 다음에 inference를 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7vlGS2aRNYLR"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(interpreter):\n",
    "    global test_images\n",
    "    \n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    input_index = input_details[\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    for test_image in tqdm(test_images):\n",
    "        # quantize input\n",
    "        if input_details['dtype'] == np.uint8:\n",
    "            input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "            test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "        # Pre-processing: add batch dimension\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Post-processing: remove batch dimension and find the digit with highest\n",
    "        # probability.\n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "        prediction_digits.append(digit)\n",
    "        \n",
    "    print()\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    prediction_digits = np.array(prediction_digits)\n",
    "    accuracy = (prediction_digits == test_labels).mean()\n",
    "    return accuracy, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itGxyCSJNYLR"
   },
   "source": [
    "tflite 모델로 inference를 하려면 [`Interpreter`](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter)를 이용한다. `model_content` 대신에 `model_path`를 이용하면 tflite 파일을 불러올 수도 있다.  \n",
    "inference를 하기 위해서는  \n",
    "interpreter 생성 &#8594; `allocate_tensor` &#8594; `set_tensor` &#8594; `invoke` 순으로 실행한다.  \n",
    "지금 코드는 inference 할 때 image를 하나씩 처리하는데, batch 단위로 처리하려면 `resize_tensor_input`을 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DSsSg4iZNYLR",
    "outputId": "43f11f9c-9cd7-48cd-92da-dc984d1f5098",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10000/10000 [00:03<00:00, 2549.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [01:21<00:00, 122.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 32bit float tflite 모델로 interpreter 만듦\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "\n",
    "# inference 할 때 input batch 수를 설정하하는 법\n",
    "# input_details = interpreter.get_input_details()[0]\n",
    "# interpreter.resize_tensor_input(input_details[\"index\"], [10, 28, 28]) # <-- 추가\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "tflite_test_accuracy, _ = evaluate_model(interpreter)\n",
    "\n",
    "# 8bit integer tflite\n",
    "post_quant_interpreter = tf.lite.Interpreter(model_content=post_quant_tflite_model)\n",
    "post_quant_interpreter.allocate_tensors()\n",
    "post_quant_tflite_test_accuracy, output = evaluate_model(post_quant_interpreter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDW5NAJYNYLS"
   },
   "source": [
    "`get_input_details`, `get_output_details`를 통해 input과 output의 type 및 quantization 정보를 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dDQavQx9NYLS",
    "outputId": "30741f15-a887-44b9-d6f1-845ade5aa13d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_input_1:0',\n",
       "  'index': 0,\n",
       "  'shape': array([ 1, 28, 28]),\n",
       "  'shape_signature': array([-1, 28, 28]),\n",
       "  'dtype': numpy.uint8,\n",
       "  'quantization': (0.003921568859368563, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00392157], dtype=float32),\n",
       "   'zero_points': array([0]),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_quant_interpreter.get_input_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "c-7djjwJNYLS",
    "outputId": "dd07ed79-36bc-472f-c5dc-110ee9028a99",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'StatefulPartitionedCall:0',\n",
       "  'index': 24,\n",
       "  'shape': array([ 1, 10]),\n",
       "  'shape_signature': array([-1, 10]),\n",
       "  'dtype': numpy.uint8,\n",
       "  'quantization': (0.00390625, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
       "   'zero_points': array([0]),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_quant_interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ISy7LIRNYLS"
   },
   "source": [
    "실제로 inference output이 정수임을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GcRoicNTNYLS",
    "outputId": "3701564a-5d63-4584-a113-fdc437f9a419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   0   1   0   0 222   0  25   6   1]]\n"
     ]
    }
   ],
   "source": [
    "print(output())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "je5MmqUlNYLS"
   },
   "source": [
    "float 32 모델과 accuracy를 비교해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "O03t5FXANYLS",
    "outputId": "96fa0e05-3ac0-4605-bdeb-5661f8cd732a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite test accuracy: 0.9067\n",
      "Post-training Quant TFLite test_accuracy: 0.9072\n"
     ]
    }
   ],
   "source": [
    "print('TFLite test accuracy:', tflite_test_accuracy)\n",
    "print('Post-training Quant TFLite test_accuracy:', post_quant_tflite_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fs85NIcbNYLS"
   },
   "source": [
    "모델을 파일로 저장해서 실제로 모델 크기가 줄어들었는지 확인해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "LMen-kkDNYLT",
    "outputId": "fb21aff4-4803-40ba-bf54-180cd2821315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model: 0.082 MB\n",
      "Quantized model: 0.025 MB\n",
      "Quantized model: 0.31 x Float model size\n"
     ]
    }
   ],
   "source": [
    "# create temp file\n",
    "_, tflite_file = tempfile.mkstemp('.tflite')\n",
    "_, post_quant_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "\n",
    "with open(tflite_file, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "with open(post_quant_tflite_file, 'wb') as f:\n",
    "    f.write(post_quant_tflite_model)\n",
    "\n",
    "\n",
    "tflite_size = os.path.getsize(tflite_file) / float(2**20)\n",
    "quantized_size = os.path.getsize(post_quant_tflite_file) / float(2**20)\n",
    "    \n",
    "print(f\"Float model: {tflite_size:.03f} MB\")\n",
    "print(f\"Quantized model: {quantized_size:.03f} MB\")\n",
    "print(f\"Quantized model: {quantized_size/tflite_size:.02f} x Float model size\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZxD5wAINYLT"
   },
   "source": [
    "### 실습: TensorFlow에서 제공하는 pretrain model 가져와서 post-training quantization 해보기\n",
    "\n",
    "TensorFlow에서는 여러 pretrained model을 제공한다. 아무거나 하나 가져와서 post-training quantization을 해보자.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VFCVsuWpNYLT"
   },
   "outputs": [],
   "source": [
    "# pretrain 모델 import 하기.\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NS6lHBAYNYLT"
   },
   "source": [
    "다음의 test image를 이용할 것이다. quantization을 할 때, representative data로는 이 이미지 하나만 이용해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zCrlaZCiNYLT",
    "outputId": "7a9ccb46-3685-47b9-cc7f-d3bf824b1ac9",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=224x224 at 0x170DD6ED4C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://cdn.education.com/files/526001_527000/526114/file_526114.jpg\"\n",
    "os.system(\"curl \" + url + \" > balloon.jpg\")\n",
    "img_path = 'balloon.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "display(img)\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x) # 각 모델을 학습할 때 preprocess했던 방법을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGThGpRgNYLT"
   },
   "source": [
    "evaluate 할 때는 다음 함수를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "50pX4XCyNYLT"
   },
   "outputs": [],
   "source": [
    "def evaluate_model_practice(interpreter, test_images):\n",
    "    \n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    input_index = input_details[\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    for test_image in tqdm(test_images):\n",
    "        # quantize input\n",
    "        if input_details['dtype'] == np.uint8:\n",
    "            input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "            test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "        # Pre-processing: add batch dimension\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        preds = interpreter.tensor(output_index)\n",
    "        preds = np.expand_dims(preds()[0], axis=0)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fiTLAk_ONYLT",
    "outputId": "819a97da-45e8-4aeb-d221-7bb20a4d9a36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "Predicted: [('n02782093', 'balloon', 0.9515529), ('n04023962', 'punching_bag', 0.03351856), ('n03544143', 'hourglass', 0.001822629)]\n"
     ]
    }
   ],
   "source": [
    "# 1. import한 pretrain 모델 가져오기\n",
    "imagenet = ResNet50(weights='imagenet')\n",
    "\n",
    "preds = imagenet.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zy8XYQknNYLT"
   },
   "source": [
    "크기 비교를 위한 32bit float tflite 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sNU42fGeNYLT",
    "outputId": "e804d0f7-05a9-466f-f47b-fb41a3afba72"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmpdqyd_ev5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmpdqyd_ev5\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(imagenet)\n",
    "imagenet_tflite = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Vl6Tg9UwNYLU",
    "outputId": "bb280c0c-cb87-4de9-9474-3ddc8da1ddbc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmp4pnd9lek\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmp4pnd9lek\\assets\n",
      "C:\\Users\\steve\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    }
   ],
   "source": [
    "# 2. full integer post-training quantization\n",
    "import numpy as np\n",
    "def representative_data_gen():\n",
    "    yield [x] # test할 이미지 1개만 사용\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(imagenet)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "post_quant_imagenet_tflite = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TsexvPr1NYLU",
    "outputId": "d20d7440-256b-4f4a-b04e-88522fb15f0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:27<00:00, 27.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n02782093', 'balloon', 239), ('n04023962', 'punching_bag', 13), ('n03544143', 'hourglass', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. make interpreter and do inference\n",
    "# tflite 모델로 interpreter 만듦\n",
    "post_quant_imagenet_interpreter = tf.lite.Interpreter(model_content=post_quant_imagenet_tflite)\n",
    "\n",
    "post_quant_imagenet_interpreter.allocate_tensors()\n",
    "\n",
    "preds = evaluate_model_practice(post_quant_imagenet_interpreter, x)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "i82UGHeONYLU",
    "outputId": "5d3f0897-b103-407a-976b-05f0e596d8c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model: 97.430 MB\n",
      "Quantized model: 25.079 MB\n",
      "Quantized model: 0.26 x Float model size\n"
     ]
    }
   ],
   "source": [
    "# Measure sizes of models.\n",
    "_, imagenet_tflite_file = tempfile.mkstemp('.tflite')\n",
    "_, post_quant_imagenet_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "\n",
    "with open(tflite_file, 'wb') as f:\n",
    "    f.write(imagenet_tflite)\n",
    "\n",
    "with open(post_quant_tflite_file, 'wb') as f:\n",
    "    f.write(post_quant_imagenet_tflite)\n",
    "\n",
    "\n",
    "tflite_qat_size = os.path.getsize(tflite_file) / float(2**20)\n",
    "quantized_size = os.path.getsize(post_quant_tflite_file) / float(2**20)\n",
    "    \n",
    "print(f\"Float model: {tflite_qat_size:.03f} MB\")\n",
    "print(f\"Quantized model: {quantized_size:.03f} MB\")\n",
    "print(f\"Quantized model: {quantized_size/tflite_qat_size:.02f} x Float model size\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeVMAXWONYLU"
   },
   "source": [
    "### 1-2. [Quantization Aware Training(QAT)](https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html)\n",
    "\n",
    "수업 시간에 배웠 듯이 QAT는 나중에 quantization을 할 것이라는 것을 가정하고 학습하는 것이다. scratch부터 학습해도 되지만, 이미 학습된 모델을 fine tuning 하는 것이 좋다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "In_3zJVhNYLU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\steve\\.conda\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\steve\\.conda\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\steve\\.conda\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\steve\\.conda\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\steve\\.conda\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\steve\\.conda\\envs\\tf\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "XVtLikZDNYLU"
   },
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2UtsSzgNYLU"
   },
   "source": [
    "### Quantize whole model\n",
    "\n",
    "Edge TPU와 같이 fully quantized 모델이 필요한 경우에는 전체 모델에 대해 QAT를 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NFDWfg8NYLU"
   },
   "source": [
    "위에서 학습한 float32모델로 QAT를 해볼 것이다.`quantize_model`함수를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "9mdw7xLDNYLU",
    "outputId": "73563958-d821-424d-af6c-aa13dae53d88",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLay  (None, 28, 28)           3         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " quant_reshape (QuantizeWrap  (None, 28, 28, 1)        1         \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_conv2d (QuantizeWrapp  (None, 28, 28, 16)       195       \n",
      " erV2)                                                           \n",
      "                                                                 \n",
      " quant_max_pooling2d (Quanti  (None, 14, 14, 16)       1         \n",
      " zeWrapperV2)                                                    \n",
      "                                                                 \n",
      " quant_conv2d_1 (QuantizeWra  (None, 14, 14, 32)       4707      \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_max_pooling2d_1 (Quan  (None, 7, 7, 32)         1         \n",
      " tizeWrapperV2)                                                  \n",
      "                                                                 \n",
      " quant_flatten (QuantizeWrap  (None, 1568)             1         \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_dense (QuantizeWrappe  (None, 10)               15695     \n",
      " rV2)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,604\n",
      "Trainable params: 20,490\n",
      "Non-trainable params: 114\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "quant_aware_model = tfmot.quantization.keras.quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "quant_aware_model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWeJ62xPNYLU"
   },
   "source": [
    "layer에 quant가 붙은 것을 확인할 수 있다. 그 다음으로는, data의 일부를 이용해서 fine tuning 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "febVafykNYLU",
    "outputId": "e6f59317-2581-428e-ee5a-6a9cc84f20cd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "108/108 [==============================] - 7s 56ms/step - loss: 0.1597 - accuracy: 0.9440 - val_loss: 0.2357 - val_accuracy: 0.9170\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 6s 54ms/step - loss: 0.1537 - accuracy: 0.9453 - val_loss: 0.2372 - val_accuracy: 0.9158\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 6s 54ms/step - loss: 0.1494 - accuracy: 0.9470 - val_loss: 0.2382 - val_accuracy: 0.9152\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 6s 54ms/step - loss: 0.1493 - accuracy: 0.9466 - val_loss: 0.2405 - val_accuracy: 0.9148\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 6s 54ms/step - loss: 0.1457 - accuracy: 0.9489 - val_loss: 0.2396 - val_accuracy: 0.9153\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 6s 54ms/step - loss: 0.1428 - accuracy: 0.9497 - val_loss: 0.2455 - val_accuracy: 0.9162\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 6s 54ms/step - loss: 0.1403 - accuracy: 0.9503 - val_loss: 0.2377 - val_accuracy: 0.9165\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 6s 54ms/step - loss: 0.1387 - accuracy: 0.9510 - val_loss: 0.2435 - val_accuracy: 0.9152\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 6s 54ms/step - loss: 0.1379 - accuracy: 0.9514 - val_loss: 0.2428 - val_accuracy: 0.9155\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 6s 56ms/step - loss: 0.1361 - accuracy: 0.9522 - val_loss: 0.2471 - val_accuracy: 0.9158\n",
      "Epoch 1/10\n",
      "108/108 [==============================] - 3s 28ms/step - loss: 0.1563 - accuracy: 0.9450 - val_loss: 0.2328 - val_accuracy: 0.9172\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 3s 28ms/step - loss: 0.1506 - accuracy: 0.9473 - val_loss: 0.2342 - val_accuracy: 0.9162\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 3s 26ms/step - loss: 0.1487 - accuracy: 0.9476 - val_loss: 0.2344 - val_accuracy: 0.9157\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 3s 26ms/step - loss: 0.1481 - accuracy: 0.9477 - val_loss: 0.2350 - val_accuracy: 0.9165\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 3s 28ms/step - loss: 0.1466 - accuracy: 0.9486 - val_loss: 0.2349 - val_accuracy: 0.9163\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 3s 27ms/step - loss: 0.1453 - accuracy: 0.9489 - val_loss: 0.2355 - val_accuracy: 0.9175\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 3s 27ms/step - loss: 0.1448 - accuracy: 0.9494 - val_loss: 0.2357 - val_accuracy: 0.9172\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 3s 27ms/step - loss: 0.1432 - accuracy: 0.9500 - val_loss: 0.2339 - val_accuracy: 0.9168\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 3s 26ms/step - loss: 0.1430 - accuracy: 0.9495 - val_loss: 0.2368 - val_accuracy: 0.9162\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 3s 27ms/step - loss: 0.1414 - accuracy: 0.9505 - val_loss: 0.2370 - val_accuracy: 0.9160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x170e3437790>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data 일부를 이용해서 fine tuning\n",
    "train_images_subset = train_images[0:1000] # out of 60000\n",
    "train_labels_subset = train_labels[0:1000]\n",
    "\n",
    "quant_aware_model.fit(train_images, train_labels, batch_size=500, epochs=10, validation_split=0.1)\n",
    "model.fit(train_images, train_labels, batch_size=500, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSRDH6ZSNYLU"
   },
   "source": [
    "아직 quantization을 하지 않았으므로 weight은 실수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ERulZV21NYLU",
    "outputId": "f433e826-5220-4dc3-ed23-f55406ec6988",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[-2.97805928e-02, -7.43684113e-01, -2.74659488e-02,\n",
       "           -3.36150527e-01, -1.51646242e-01,  2.80483454e-01,\n",
       "           -3.75585333e-02,  4.74252820e-01, -1.58959344e-01,\n",
       "            3.64487827e-01,  7.50890374e-02,  1.80487379e-01,\n",
       "           -4.17069763e-01,  5.73657341e-02, -4.25913185e-01,\n",
       "            6.07960746e-02]],\n",
       " \n",
       "         [[ 4.22853321e-01,  4.99973953e-01, -5.77079117e-01,\n",
       "            1.63602322e-01, -3.52849871e-01,  5.39748073e-01,\n",
       "            3.13947350e-01,  5.41177243e-02, -3.15265089e-01,\n",
       "            4.79595333e-01, -1.02142207e-01, -4.15292531e-01,\n",
       "           -2.87013054e-02,  2.50699997e-01, -6.11357868e-01,\n",
       "           -8.99027511e-02]],\n",
       " \n",
       "         [[-7.17292652e-02,  1.78618535e-01, -2.41443440e-01,\n",
       "            2.24919006e-01,  5.02402596e-02, -6.94538057e-01,\n",
       "           -2.02082738e-01, -5.22242486e-01,  4.98584688e-01,\n",
       "            3.93959075e-01,  8.95497389e-03,  3.00242126e-01,\n",
       "            1.50624916e-01,  2.65799791e-01, -1.46604791e-01,\n",
       "           -7.11415634e-02]]],\n",
       " \n",
       " \n",
       "        [[[ 5.29776141e-02,  3.08418095e-01,  6.41792357e-01,\n",
       "           -6.32075250e-01,  3.93101484e-01,  2.33841315e-01,\n",
       "           -2.22679958e-01,  2.02334195e-01,  5.45648113e-02,\n",
       "            1.39931560e-01, -7.84161761e-02, -3.38099360e-01,\n",
       "           -5.86386859e-01, -3.21117304e-02,  3.54727149e-01,\n",
       "           -5.64553775e-02]],\n",
       " \n",
       "         [[-2.15990059e-02,  2.50625670e-01,  6.36176988e-02,\n",
       "            4.10803229e-01,  2.02058122e-01,  2.58284509e-01,\n",
       "            1.09145567e-01, -2.20223777e-02, -4.24716949e-01,\n",
       "           -1.47864252e-01,  1.24147825e-01, -2.20476210e-01,\n",
       "           -1.25056073e-01,  1.87352106e-01,  3.74921620e-01,\n",
       "            4.08540696e-01]],\n",
       " \n",
       "         [[-2.19418430e+00, -6.39912337e-02,  1.18381746e-01,\n",
       "            1.97732553e-01, -4.10476923e-01, -2.28948995e-01,\n",
       "            4.53945249e-01, -1.81547463e-01,  3.42450231e-01,\n",
       "           -1.88950717e-01,  1.48650095e-01,  5.27823508e-01,\n",
       "            2.08693340e-01,  1.08162418e-01,  2.96067178e-01,\n",
       "           -6.16785474e-02]]],\n",
       " \n",
       " \n",
       "        [[[ 2.56848425e-01,  4.74823952e-01,  4.59408224e-01,\n",
       "           -2.60803699e-01, -1.43525407e-01,  2.66877562e-01,\n",
       "           -1.24529958e-01,  7.09825009e-02,  2.96956897e-01,\n",
       "           -2.43422300e-01,  1.84861347e-01, -4.21301842e-01,\n",
       "            2.96156168e-01, -1.21224314e-01,  6.29922599e-02,\n",
       "           -4.73084867e-01]],\n",
       " \n",
       "         [[-7.02704251e-01, -5.51396489e-01, -3.40292394e-01,\n",
       "            2.62337744e-01,  3.81009161e-01, -2.23676026e-01,\n",
       "           -4.36968774e-01, -1.21843174e-01, -1.66733682e-01,\n",
       "           -5.41026413e-01,  2.53711730e-01,  2.33094707e-01,\n",
       "            2.00024694e-01, -7.11484998e-02, -2.48354018e-01,\n",
       "            2.65889227e-01]],\n",
       " \n",
       "         [[-2.43265343e+00, -3.54748905e-01, -6.75887913e-02,\n",
       "           -5.22711256e-04,  1.96622103e-01, -2.40664735e-01,\n",
       "            2.81311721e-01,  7.51833320e-02, -1.26123250e-01,\n",
       "           -3.05313081e-01,  5.65091968e-02,  1.61916405e-01,\n",
       "            2.76342481e-01, -5.85289523e-02,  2.13585988e-01,\n",
       "            1.25739694e-01]]]], dtype=float32),\n",
       " array([ 0.20930149,  0.00711821,  0.00880488, -0.0046264 ,  0.00398198,\n",
       "        -0.01826496, -0.02187598, -0.00026163,  0.00670902, -0.0112316 ,\n",
       "        -0.04178863,  0.00905187,  0.06640767, -0.00070406,  0.0795297 ,\n",
       "        -0.0170257 ], dtype=float32),\n",
       " -1,\n",
       " array([-2.4325767 , -0.743827  , -0.6416965 , -0.63208836, -0.41081828,\n",
       "        -0.69466937, -0.45436475, -0.5221288 , -0.49856853, -0.5411715 ,\n",
       "        -0.25326955, -0.5277887 , -0.58620584, -0.2657407 , -0.61171055,\n",
       "        -0.47310513], dtype=float32),\n",
       " array([2.4325767 , 0.743827  , 0.6416965 , 0.63208836, 0.41081828,\n",
       "        0.69466937, 0.45436475, 0.5221288 , 0.49856853, 0.5411715 ,\n",
       "        0.25326955, 0.5277887 , 0.58620584, 0.2657407 , 0.61171055,\n",
       "        0.47310513], dtype=float32),\n",
       " -2.03647,\n",
       " 2.9824297]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_aware_model.layers[2].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPynQSIBNYLV"
   },
   "source": [
    "실수 weight이므로 QAT를 한다고 해서 accuracy가 떨어지지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "7Hn8ztCpNYLV",
    "outputId": "fc5a37b2-13a5-45c9-cb95-6d354e55dee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.9157000184059143\n",
      "QAT test accuracy: 0.9136000275611877\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "_, quant_aware_model_accuracy = quant_aware_model.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('QAT test accuracy:', quant_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qhw2P9JSNYLV"
   },
   "source": [
    "이제 QAT로 학습한 모델을 quantization 해보자. post-training quantization과 비슷하게 TFLiteConverter를 이용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ix9ebDT8NYLV",
    "outputId": "0e172d49-59ad-4f20-e445-4e0fd3dcfc38",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as reshape_layer_call_fn, reshape_layer_call_and_return_conditional_losses, conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op while saving (showing 5 of 12). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmp2crpxhln\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmp2crpxhln\\assets\n",
      "C:\\Users\\steve\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8 # or tf.int8\n",
    "converter.inference_output_type = tf.uint8 # or tf.int8\n",
    "\n",
    "\n",
    "QAT_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrsYcEatNYLV"
   },
   "source": [
    "quantization을 해도 accuracy가 떨어지지 않는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "pkzQZbvpNYLV",
    "outputId": "d7265216-219d-4f7b-d719-1193f274e388"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [01:15<00:00, 132.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline test accuracy: 0.9157000184059143\n",
      "Post-training Quantized test accuracy: 0.9072\n",
      "Quantization-Aware Training test accuracy: 0.9133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "QAT_interpreter = tf.lite.Interpreter(model_content=QAT_tflite_model)\n",
    "QAT_interpreter.allocate_tensors()\n",
    "\n",
    "QAT_test_accuracy, output = evaluate_model(QAT_interpreter)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Post-training Quantized test accuracy:', post_quant_tflite_test_accuracy)\n",
    "print('Quantization-Aware Training test accuracy:', QAT_test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9HCVzztNYLV"
   },
   "source": [
    "마찬가지로 크기가 1/4로 줄어든 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "BEP3F5oGNYLV",
    "outputId": "527cf46f-8e72-4f59-df08-9dc1d98299b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float tflite model: 0.082 MB\n",
      "Quantized QAT model: 0.026 MB\n",
      "Quantized QAT model: 0.32 x Float model size\n"
     ]
    }
   ],
   "source": [
    "# create temp file\n",
    "_, QAT_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(QAT_tflite_file, 'wb') as f:\n",
    "    f.write(QAT_tflite_model)\n",
    "    \n",
    "    \n",
    "quantized_size = os.path.getsize(QAT_tflite_file) / float(2**20)\n",
    "\n",
    "print(f\"Float tflite model: {tflite_size:.03f} MB\")\n",
    "print(f\"Quantized QAT model: {quantized_size:.03f} MB\")\n",
    "print(f\"Quantized QAT model: {quantized_size/tflite_size:.02f} x Float model size\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSmrspWkNYLV"
   },
   "source": [
    "### Quantize some layers\n",
    "\n",
    "Edge TPU 등 특별한 하드웨어가 아닌 경우에는 선택적으로 layer를 quantization할 수도 있다.\n",
    "\n",
    "**Tips for better accuracy**\n",
    "* from scratch보다는 fine tuning\n",
    "* 뒤쪽 layer를 quantization 하기\n",
    "* 특별히 중요한 layer는 quantization 피하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3w9grImZNYLV"
   },
   "source": [
    "`quantize_annotate_layer` 함수를 이용해서 특정 layer를 quantization할 것이라고 표시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "uFUH1L8oNYLV",
    "outputId": "47fb86dc-ad59-4107-eccd-5b4b82bbb166"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "v8DAiFDeNYLV"
   },
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def apply_quantization_to_dense(layer):\n",
    "    # if isinstance(layer, tf.keras.layers.Dense)\n",
    "    if layer.name == 'dense':\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "    else:\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l63Ds75iNYLV"
   },
   "source": [
    "`clone_model` 함수와 helper function을 이용해서 annotated model을 만들고, `quantize_apply`함수를 이용해서 QAT를 위한 모델을 만들면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "whCWT5fHNYLV",
    "outputId": "bd3163f5-a5f3-4bb4-c394-8559e558aede",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " quant_flatten (QuantizeWrap  (None, 1568)             1         \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_dense (QuantizeWrappe  (None, 10)               15695     \n",
      " rV2)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,496\n",
      "Trainable params: 20,490\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "annotated_model = tf.keras.models.clone_model(\n",
    "    model,\n",
    "    clone_function=apply_quantization_to_dense,\n",
    ")\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsgEGC0GNYLV"
   },
   "source": [
    "만약 fine tuning이 아니라 처음부터 모델을 만드는 것이면 다음과 같이 모델을 만들면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxbhoMR9NYLW"
   },
   "source": [
    "**Sequential example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "qv3NwBeNNYLW",
    "outputId": "2fa297ed-d464-4b8e-ee60-069f19321be5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_1 (Reshape)         (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 12)        120       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 13, 13, 12)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " quant_flatten_1 (QuantizeWr  (None, 2028)             1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_1 (QuantizeWrap  (None, 10)               20295     \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,416\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "annotated_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(10))\n",
    "])\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tY2OlJAgNYLW"
   },
   "source": [
    "**Functional example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "h08zY1pVNYLW",
    "outputId": "afb558a7-ab6c-47cb-90d1-87f923cf6b67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 28, 28)]          0         \n",
      "                                                                 \n",
      " quant_reshape_2 (QuantizeWr  (None, 28, 28, 1)        1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv2d_3 (QuantizeWra  (None, 26, 26, 12)       147       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 13, 13, 12)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2028)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                20290     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,438\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(28, 28))\n",
    "x = tf.keras.layers.Reshape(target_shape=(28, 28, 1))(inputs)\n",
    "x = tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'))(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "outputs = tf.keras.layers.Dense(10)(x)\n",
    "\n",
    "annotated_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxfhZjYINYLW"
   },
   "source": [
    "## 2. [Pruning](https://blog.tensorflow.org/2019/05/tf-model-optimization-toolkit-pruning-API.html)\n",
    "\n",
    "pruning은 불필요한(0에 가까운) weight을 0으로 만들어 없애면서 optimize를 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P24wmByjNYLW"
   },
   "source": [
    "`tfmot.sparsity.keras.prune_low_magnitude`을 이용해서 모델을 만든다. 이 때, pruning 방식인 `tfmot.sparsity.keras.PolynomialDecay` 을 parameter로 넘겨준다. 다음과 같은 hyperparemeter가 있다.\n",
    "* `initial_sparsity`: pruning을 시작할 때의 sparsity를 몇으로 할 지\n",
    "* `final_sparsity`: pruning을 끝낼 때의 sparsity를 몇으로 할 지\n",
    "* `begin_step`: pruning을 언제부터 진행할 지 (batch 단위의 step)\n",
    "* `end_step`: pruning을 언제 끝낼 지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zn1YDlqZNYLW"
   },
   "source": [
    "pruning을 하기 전에 baseline model을 저장하겠다. (크기 비교시 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "M_huVivXNYLW",
    "outputId": "48e5b16c-b5df-44c0-ab83-01563806766f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.9157000184059143\n",
      "Saved baseline model to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmpkdn_znh2.h5\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "\n",
    "_, keras_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
    "print('Saved baseline model to:', keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "q0FjBTcbNYLW",
    "outputId": "b8839313-d4f6-4c17-c16b-acced2803ba1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_reshape  (None, 28, 28, 1)        1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d   (None, 28, 28, 16)       306       \n",
      " (PruneLowMagnitude)                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_max_poo  (None, 14, 14, 16)       1         \n",
      " ling2d (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 14, 14, 32)       9250      \n",
      " 1 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_max_poo  (None, 7, 7, 32)         1         \n",
      " ling2d_1 (PruneLowMagnitude                                     \n",
      " )                                                               \n",
      "                                                                 \n",
      " prune_low_magnitude_flatten  (None, 1568)             1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_dense (  (None, 10)               31372     \n",
      " PruneLowMagnitude)                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,932\n",
      "Trainable params: 20,490\n",
      "Non-trainable params: 20,442\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "num_images = train_images.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtvDVmnfNYLW"
   },
   "source": [
    "`fit`을 할 때, `tfmot.sparsity.keras.UpdatePruningStep`을 callback으로 불러야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "F5bI6NLfNYLW",
    "outputId": "1fabe947-90cd-45c6-b4e0-15317c9b972e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/422 [..............................] - ETA: 4s - loss: 0.1437 - accuracy: 0.9557   WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0096s vs `on_train_batch_end` time: 0.0198s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0096s vs `on_train_batch_end` time: 0.0198s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422/422 [==============================] - 13s 14ms/step - loss: 0.2628 - accuracy: 0.9038 - val_loss: 0.3353 - val_accuracy: 0.8822\n",
      "Epoch 2/2\n",
      "422/422 [==============================] - 5s 12ms/step - loss: 0.2813 - accuracy: 0.8979 - val_loss: 0.2912 - val_accuracy: 0.8970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x170e39fc820>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "\n",
    "model_for_pruning.fit(train_images, train_labels,\n",
    "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Fpn4uJJNYLW"
   },
   "source": [
    "baseline 모델과 accuracy를 비교해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "XTqyfCXZNYLW",
    "outputId": "99107db0-9e79-4e6c-fffa-8cd822ee3394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.9157000184059143\n",
      "Pruned test accuracy: 0.8914999961853027\n"
     ]
    }
   ],
   "source": [
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy) \n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "313XRqaQNYLW"
   },
   "source": [
    "pruned model의 크기가 줄어들었는 지 확인해 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQDN-jBoNYLW"
   },
   "source": [
    "위에 summary를 보면 pruning을 하기 위해 non-trainable parameter가 생긴 것을 알 수 있다. `tfmot.sparsity.keras.strip_pruning`을 이용해서 pruning에 사용한 variable을 제거해 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "T0qJJJmNNYLX",
    "outputId": "386cf11e-47bd-4756-9fb5-b11f3338e04d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned Keras model to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmp7ewcmwb9.h5\n"
     ]
    }
   ],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', pruned_keras_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGuJf_g_NYLX"
   },
   "source": [
    "tflite 으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "U_ue9-QXNYLX",
    "outputId": "ea391b13-55c7-44b3-8ab6-6d97dded9bf5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmpkz7wp4_2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmpkz7wp4_2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned TFLite model to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmp_4pclva_.tflite\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "    f.write(pruned_tflite_model)\n",
    "\n",
    "print('Saved pruned TFLite model to:', pruned_tflite_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fY3QcdaPNYLX"
   },
   "source": [
    "pruning을 해도 weight matrix의 크기는 그대로이다. 하지만 matrix의 값이 대부분 0이기 때문에 감소된 크기를 확인하려면 실제로 zip파일 등으로 압축하는 과정을 거쳐야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Tdd6Qkm1NYLX"
   },
   "outputs": [],
   "source": [
    "def get_gzipped_model_size(file):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(file)\n",
    "    return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "1IbZTCZ6NYLX",
    "outputId": "bea9626c-54ff-4b74-d3ac-7c2fc2eaabb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of gzipped baseline Keras model: 78969.00 bytes\n",
      "Size of gzipped pruned Keras model: 26686.00 bytes\n",
      "Size of gzipped pruned TFlite model: 25419.00 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
    "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_Sk8uNPNYLX"
   },
   "source": [
    "### Combine pruning and quantization\n",
    "pruning을 한 다음에 post-training quantization까지 하면 크기를 더 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "MO0PrS93NYLX",
    "outputId": "4de60404-df84-4b4e-a5f5-aa24ac07df44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmph1dadejl\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmph1dadejl\\assets\n",
      "C:\\Users\\steve\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved quantized and pruned TFLite model to: C:\\Users\\steve\\AppData\\Local\\Temp\\tmpdwkcziqh.tflite\n",
      "Size of gzipped baseline Keras model: 78969.00 bytes\n",
      "Size of gzipped pruned and quantized TFlite model: 9704.00 bytes\n",
      "Size of optimized model is 0.12 of the baseline model\n"
     ]
    }
   ],
   "source": [
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "quantized_and_pruned_tflite_model = converter.convert()\n",
    "\n",
    "_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
    "    f.write(quantized_and_pruned_tflite_model)\n",
    "\n",
    "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
    "\n",
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))\n",
    "print(f\"Size of optimized model is {get_gzipped_model_size(quantized_and_pruned_tflite_file)/get_gzipped_model_size(keras_file):.02f} of the baseline model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmunG-WdNYLX"
   },
   "source": [
    "baseline 과 accuracy를 비교해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "jZXMU5h1NYLX",
    "outputId": "8af5bb7f-d0fa-455f-fe58-598015a48137",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [01:15<00:00, 132.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline test accuracy: 0.9157000184059143\n",
      "Pruned and quantized TFLite test_accuracy: 0.8917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy, _ = evaluate_model(interpreter)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_AasKQXNYLX"
   },
   "source": [
    "QAT와 비슷하게, pruning도 layer를 선택적으로 pruning 할 수 있다. 이 부분은 [document](https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide)를 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Practice_Optimization (4).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
